# Vehicle Insurance Prediction Data Pipeline

## ğŸš€ Project Overview

This project is an end-to-end machine learning pipeline for predicting vehicle insurance outcomes. It follows modern MLOps practices with a modular architecture, cloud integration, and containerization for seamless deployment.

## ğŸ“‚ Project Structure

```
src/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ components/               # Pipeline components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”œâ”€â”€ model_evaluation.py
â”‚   â””â”€â”€ model_pusher.py
â”œâ”€â”€ configuration/            # Cloud and DB configurations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mongo_db_connection.py
â”‚   â””â”€â”€ aws_connection.py
â”œâ”€â”€ cloud_storage/            # Cloud storage operations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ aws_storage.py
â”œâ”€â”€ data_access/              # Data access layer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ proj1_data.py
â”œâ”€â”€ constants/                # Project constants
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ entity/                   # Configuration and artifact entities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config_entity.py
â”‚   â”œâ”€â”€ artifact_entity.py
â”‚   â”œâ”€â”€ estimator.py
â”‚   â””â”€â”€ s3_estimator.py
â”œâ”€â”€ exception/                # Custom exceptions
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ logger/                   # Logging configuration
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ pipline/                  # Training and prediction pipelines
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ training_pipeline.py
â”‚   â””â”€â”€ prediction_pipeline.py
â”œâ”€â”€ utils/                    # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main_utils.py
config/                       # Configuration files
â”‚   â”œâ”€â”€ model.yaml
â”‚   â””â”€â”€ schema.yaml
app.py                        # FastAPI application
requirements.txt              # Python dependencies
Dockerfile                    # Container configuration
.dockerignore                 # Docker ignore rules
demo.py                       # Demonstration script
setup.py                      # Package configuration
pyproject.toml                # Build system configuration
```

## ğŸ› ï¸ Technology Stack

### â˜ï¸ Cloud Services
- **AWS S3**: For model and artifact storage
- **AWS EC2**: For deployment and hosting
- **MongoDB Atlas**: For data storage and retrieval

### ğŸ Python Ecosystem
- **FastAPI**: For building the web application
- **Pydantic**: For data validation
- **PyMongo**: For MongoDB interactions
- **Boto3**: For AWS services integration

### ğŸš¢ Containerization & Deployment
- **Docker**: For containerization
- **Docker Compose**: For multi-container management

### ğŸ“Š Machine Learning
- **Scikit-learn**: For model training and evaluation
- **PyYAML**: For configuration management

## ğŸ” Key Features

1. **Modular Pipeline Architecture**: Each ML step (ingestion, validation, transformation, etc.) is encapsulated in separate components
2. **Cloud-Native Design**: Built from the ground up for cloud deployment with AWS integration
3. **Configuration Management**: YAML-based configuration for models and data schemas
4. **Containerized Deployment**: Ready for Docker-based deployment
5. **Comprehensive Logging**: Built-in logging system for tracking pipeline execution
6. **REST API Interface**: FastAPI-powered endpoints for predictions

## ğŸ—ï¸ Pipeline Workflow

1. **Data Ingestion**: Pull data from MongoDB Atlas
2. **Data Validation**: Ensure data quality and schema compliance
3. **Data Transformation**: Prepare data for modeling
4. **Model Training**: Train machine learning models
5. **Model Evaluation**: Assess model performance
6. **Model Deployment**: Push validated models to AWS S3
7. **API Serving**: Make predictions available via FastAPI endpoints

## ğŸš€ Getting Started

1. Set up your AWS credentials and MongoDB Atlas connection
2. Configure the YAML files in the `config/` directory
3. Build and run the Docker container
4. Access the FastAPI application for training and predictions

This project demonstrates a production-grade ML pipeline following best practices for maintainability, scalability, and reliability in cloud environments.
